{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e29ed7f",
   "metadata": {},
   "source": [
    "NLTK</br>\n",
    "Natural Language Tool Kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e75f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ujwal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ujwal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ujwal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens: ['the', 'quick', 'brown', 'foxes', 'are', 'jumping', 'over', 'the', 'lazy', 'dogs']\n",
      "Cleaned Tokens: ['quick', 'brown', 'foxes', 'jumping', 'lazy', 'dogs']\n",
      "Lemmatized Tokens: ['quick', 'brown', 'fox', 'jumping', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet') \n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "text = \"The quick brown foxes are jumping over the lazy dogs\"\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "print(f\"Original Tokens: {tokens}\")\n",
    "print(f\"Cleaned Tokens: {filtered_tokens}\")\n",
    "print(f\"Lemmatized Tokens: {lemmatized_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd5939b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Alexandor', 'NNP'), ('Albon', 'NNP'), ('was', 'VBD'), ('the', 'DT'), ('rookie', 'NN'), ('dirver', 'NN'), ('in', 'IN'), ('Formula', 'NNP'), ('one', 'NN'), ('who', 'WP'), ('was', 'VBD'), ('very', 'RB'), ('successful', 'JJ'), ('at', 'IN'), ('the', 'DT'), ('start', 'NN'), ('then', 'RB'), ('ruined', 'VBD'), ('when', 'WRB'), ('Hamilton', 'NNP'), ('struck', 'VBD'), ('his', 'PRP$'), ('mercedes', 'NNS'), ('when', 'WRB'), ('Albon', 'NNP'), ('was', 'VBD'), ('about', 'IN'), ('to', 'TO'), ('win', 'VB'), ('in', 'IN'), ('Austria', 'NNP')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ujwal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\ujwal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets') \n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"\"\"Alexandor Albon was the rookie dirver in Formula one who was very successful at the start then ruined when\n",
    "Hamilton struck his mercedes when Albon was about to win in Austria\"\"\"\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "print(pos_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ff8e0e",
   "metadata": {},
   "source": [
    "## Spacy\n",
    "What it is: A pipeline of functions where the output of one component is the input to the next. Each component performs a specific task and adds data to the Doc object.\n",
    "\n",
    "A typical default pipeline might look like this:\n",
    "tok2vec -> tagger -> parser -> ner -> attribute_ruler -> lemmatizer\n",
    "\n",
    "tok2vec: Creates vector representations of words.\n",
    "\n",
    "tagger: Performs Part-of-Speech (POS) tagging (doc[0].pos_).\n",
    "\n",
    "parser: Performs dependency parsing (analyzing grammatical relationships).\n",
    "\n",
    "ner: Performs Named Entity Recognition (doc.ents).\n",
    "\n",
    "lemmatizer: Finds the base form of words (doc[0].lemma_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca812fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens [Alexandor, Albon, was, the, rookie, dirver, in, Formula, one, who, was, very, successful, at, the, start, then, ruined, when, \n",
      ", Hamilton, struck, his, mercedes, when, Albon, was, about, to, win, in, Austria]\n",
      " Lemmatization and Stop Word Removal\n",
      "Token: Alexandor, Lemma: Alexandor\n",
      "Token: Albon, Lemma: Albon\n",
      "Token: rookie, Lemma: rookie\n",
      "Token: dirver, Lemma: dirver\n",
      "Token: Formula, Lemma: Formula\n",
      "Token: successful, Lemma: successful\n",
      "Token: start, Lemma: start\n",
      "Token: ruined, Lemma: ruin\n",
      "Token: \n",
      ", Lemma: \n",
      "\n",
      "Token: Hamilton, Lemma: Hamilton\n",
      "Token: struck, Lemma: strike\n",
      "Token: mercedes, Lemma: mercede\n",
      "Token: Albon, Lemma: Albon\n",
      "Token: win, Lemma: win\n",
      "Token: Austria, Lemma: Austria\n",
      "\n",
      "--- Named Entity Recognition (NER) ---\n",
      "Entity: Alexandor Albon, Label: PERSON\n",
      "Entity: Formula, Label: PERSON\n",
      "Entity: Hamilton, Label: PERSON\n",
      "Entity: Albon, Label: PERSON\n",
      "Entity: Austria, Label: GPE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"\"\"Alexandor Albon was the rookie dirver in Formula one who was very successful at the start then ruined when\n",
    "Hamilton struck his mercedes when Albon was about to win in Austria\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "tokens=[token for token in doc]\n",
    "print(\"Tokens\",tokens)\n",
    "print(\" Lemmatization and Stop Word Removal\")\n",
    "\n",
    "for token in doc:\n",
    "    # .lemma_ gives the base form, .is_stop checks if it's a stop word\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        print(f\"Token: {token.text}, Lemma: {token.lemma_}\")\n",
    "\n",
    "print(\"\\n--- Named Entity Recognition (NER) ---\")\n",
    "# The 'doc.ents' attribute directly gives you the named entities\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5284a9",
   "metadata": {},
   "source": [
    "Spacy is a preDefined processing pipline</br>\n",
    "we need to add to pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1dac893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller # type: ignore\n",
    "import wordninja #type: ignore\n",
    "from spacy.language import Language #type: ignore\n",
    "from spacy.tokens import Doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c305b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc.set_extension(\"was_corrected\", default=False)\n",
    "Doc.set_extension(\"corrected_text\", default=\"\")\n",
    "\n",
    "# Register custom extensions for the Doc object\n",
    "@Language.component(\"text_correction_component\")\n",
    "def text_correction_component(doc):\n",
    "\n",
    "    spell = Speller(lang='en')\n",
    "    \n",
    "    corrected_tokens = []\n",
    "    for token in doc:\n",
    "\n",
    "        corrected_word = spell(token.text)\n",
    "\n",
    "        split_words = wordninja.split(corrected_word)\n",
    "\n",
    "        corrected_tokens.extend(split_words)\n",
    "\n",
    "    final_text = \" \".join(corrected_tokens)\n",
    "    \n",
    "\n",
    "    if final_text != doc.text:\n",
    "        doc._.was_corrected = True\n",
    "        doc._.corrected_text = final_text\n",
    "    else:\n",
    "        doc._.was_corrected = False\n",
    "        doc._.corrected_text = doc.text\n",
    "        \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9671a27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: ['text_correction_component']\n",
      "\n",
      "Original Text: 'Thiss is a testfor mynewproject. I hope it workd.'\n",
      "Was Corrected?: True\n",
      "Corrected Text: 'This s is a ten for my new project I hope it work'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load a blank English model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Add our custom component to the pipeline\n",
    "nlp.add_pipe(\"text_correction_component\", first=True)\n",
    "\n",
    "print(f\"Pipeline: {nlp.pipe_names}\")\n",
    "\n",
    "messy_text = \"Thiss is a testfor mynewproject. I hope it workd.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(messy_text)\n",
    "\n",
    "\n",
    "# Check the results!\n",
    "print(f\"\\nOriginal Text: '{doc.text}'\")\n",
    "print(f\"Was Corrected?: {doc._.was_corrected}\")\n",
    "print(f\"Corrected Text: '{doc._.corrected_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baa43b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
