{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "114ffd92",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG)<br>\n",
    "<ul>\n",
    "Indexing<ol>\n",
    "Doccument loading<br>\n",
    "Text Splitting<br>\n",
    "Embedding<br>\n",
    "vector storeage<br></ol>\n",
    "Retirval<ol>\n",
    "query embedding<br>\n",
    "Sim search<br>\n",
    "retirve K docc K-means<br>\n",
    "Context augmentation<br></ol>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347959bb",
   "metadata": {},
   "source": [
    "### Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32922617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents from text file.\n",
      "metadata: {'source': 'data/text.txt'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders.text import TextLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "loader= TextLoader(\"data/text.txt\",encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents from text file.\")\n",
    "print(\"metadata:\", documents[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93879948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 documents from PDF file.\n",
      "metadata: {'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'title': 'Smart Water Bottle Proposal', 'author': 'ChatGPT Canvas', 'source': 'data/pdf.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"data/pdf.pdf\")\n",
    "documents = loader.load()\n",
    "print(f\"Loaded {len(documents)} documents from PDF file.\")\n",
    "print(\"metadata:\", documents[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdb42347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents from web page.\n",
      "metadata: {'source': 'https://en.wikipedia.org/wiki/William_Hanna', 'title': 'William Hanna - Wikipedia', 'language': 'en'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/William_Hanna\")\n",
    "documents = loader.load()\n",
    "print(f\"Loaded {len(documents)} documents from web page.\")\n",
    "print(\"metadata:\", documents[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01cd747",
   "metadata": {},
   "source": [
    "### Text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02fc5e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "long_text = \"\"\"\n",
    "LangChain is a powerful framework for developing applications powered by language models.\n",
    "It enables applications that are:\n",
    "1. Data-aware: connect a language model to other sources of data.\n",
    "2. Agentic: allow a language model to interact with its environment.\n",
    "\n",
    "The core idea of LangChain is to \"chain\" together different components to create more advanced use cases around LLMs.\n",
    "This includes modules for:\n",
    "- Models: LLMs, ChatModels, Embeddings\n",
    "- Prompts: PromptTemplate, ChatPromptTemplate\n",
    "- Output Parsers: StrOutputParser, JsonOutputParser, PydanticOutputParser\n",
    "- Indexes: Document Loaders, Text Splitters, Vectorstores, Retrievers\n",
    "- Chains: Combining components with LCEL\n",
    "- Agents: LLMs that can make decisions and use tools\n",
    "- Memory: Persisting state between turns\n",
    "\n",
    "LangChain is available in Python and JavaScript/TypeScript.\n",
    "It also has related projects like LangServe for deployment and LangSmith for observability.\n",
    "\"\"\"\n",
    "doc_to_split = Document(page_content=long_text, metadata={\"source\": \"internal_doc\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "257441af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split document into 13 chunks:\n",
      "Chunk 1: LangChain is a powerful framework for developing a... (metadata: {'source': 'internal_doc'})\n",
      "Chunk 2: It enables applications that are:\n",
      "1. Data-aware: c... (metadata: {'source': 'internal_doc'})\n",
      "Chunk 3: 2. Agentic: allow a language model to interact wit... (metadata: {'source': 'internal_doc'})\n",
      "Chunk 4: The core idea of LangChain is to \"chain\" together ... (metadata: {'source': 'internal_doc'})\n",
      "Chunk 5: more advanced use cases around LLMs.... (metadata: {'source': 'internal_doc'})\n",
      "Chunk 6: This includes modules for:\n",
      "- Models: LLMs, ChatMod... (metadata: {'source': 'internal_doc'})\n",
      "Chunk 7: - Prompts: PromptTemplate, ChatPromptTemplate... (metadata: {'source': 'internal_doc'})\n",
      "Chunk 8: - Output Parsers: StrOutputParser, JsonOutputParse... (metadata: {'source': 'internal_doc'})\n",
      "Chunk 9: - Indexes: Document Loaders, Text Splitters, Vecto... (metadata: {'source': 'internal_doc'})\n",
      "Chunk 10: - Chains: Combining components with LCEL\n",
      "- Agents:... (metadata: {'source': 'internal_doc'})\n",
      "Chunk 11: - Memory: Persisting state between turns... (metadata: {'source': 'internal_doc'})\n",
      "Chunk 12: LangChain is available in Python and JavaScript/Ty... (metadata: {'source': 'internal_doc'})\n",
      "Chunk 13: It also has related projects like LangServe for de... (metadata: {'source': 'internal_doc'})\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100, chunk_overlap=20,separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents([doc_to_split])\n",
    "\n",
    "print(f\"Split document into {len(chunks)} chunks:\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk.page_content[:50]}... (metadata: {chunk.metadata})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0549406a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
