{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ce79189",
   "metadata": {},
   "source": [
    "### Zero-Shot Prompting:\n",
    " You provide the instruction and the input, and the LLM generates the output without any examples. This is the simplest approach.<br>\n",
    "### Few Shot Prompting:\n",
    "Few-Shot Prompting: You provide a few examples of input-output pairs along with the instruction. This helps the LLM understand the desired format, style, or task, especially for tasks that are not immediately obvious or require a specific pattern.<br>\n",
    "**FewShotPromptTemplate(), FewShotChatMessagePromptTemplate()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bdbb549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "google_apii_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "llm= ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    google_api_key=google_apii_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b867dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant use the given exmaple for reference.\n",
      "What is the capital of France?\n",
      "The capital of France is Paris.\n",
      "What is the largest planet in our solar system?\n",
      "The largest planet in our solar system is Jupiter.\n",
      "What is the tallest mountain in the world?\n",
      "LLM Response: The tallest mountain in the world, measured from base to peak, is Mount Everest.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import FewShotChatMessagePromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import HumanMessagePromptTemplate, AIMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "    AIMessagePromptTemplate.from_template(\"{output}\")\n",
    "])\n",
    "        \n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=[\n",
    "        {\n",
    "            \"input\": \"What is the capital of France?\",\n",
    "            \"output\": \"The capital of France is Paris.\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"What is the largest planet in our solar system?\",\n",
    "            \"output\": \"The largest planet in our solar system is Jupiter.\"\n",
    "        },\n",
    "    ],\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"You are a helpful assistant use the given exmaple for reference.\"),\n",
    "    few_shot_prompt,\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "])\n",
    "input_text = \"What is the tallest mountain in the world?\"\n",
    "response = final_prompt.format_messages(input=input_text)\n",
    "for message in response:\n",
    "    print(message.content)\n",
    "    \n",
    "llm_response= llm.invoke(response)\n",
    "print(\"LLM Response:\", llm_response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee1d30f",
   "metadata": {},
   "source": [
    "## Chain of Thoughts CoT <br>\n",
    "Chain of Thought (CoT) Prompting: You instruct the LLM to show its reasoning steps before providing the final answer. This helps the LLM break down complex problems, improves accuracy on reasoning tasks, and makes the LLM's thought process transparent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e57c0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response without CoT: Here's how to solve the problem:\n",
      "\n",
      "*   **Start:** The baker has 20 cookies.\n",
      "*   **Sells:** The baker sells 12 cookies, leaving 20 - 12 = 8 cookies.\n",
      "*   **Bakes more:** The baker bakes 10 more cookies, so they have 8 + 10 = 18 cookies.\n",
      "\n",
      "**Answer:** The baker has 18 cookies now. \n",
      "\n",
      "\n",
      "Response with CoT: Okay, let's break it down:\n",
      "\n",
      "1. **Starts with:** The baker begins with 20 cookies.\n",
      "2. **Sells some:** The baker sells 12 cookies, so 20 - 12 = 8 cookies are left.\n",
      "3. **Bakes more:** The baker bakes 10 more cookies, so 8 + 10 = 18 cookies.\n",
      "\n",
      "**Answer:** The baker has 18 cookies now. \n",
      "\n",
      "\n",
      "Response with complex CoT: Okay, let's break it down:\n",
      "\n",
      "*   **A is older than B:** This means B is younger than A.\n",
      "*   **B is older than C:** This means C is younger than B.\n",
      "*   **C is older than D:** This means D is younger than C.\n",
      "\n",
      "So we have A > B > C > D (where \">\" means \"is older than\").\n",
      "\n",
      "Therefore, **D** is the youngest.\n"
     ]
    }
   ],
   "source": [
    "prompt_without_CoT = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"Question: If a baker bakes 20 cookies and sells 12, then bakes 10 more, how many cookies does the baker have now?\")\n",
    "    ])\n",
    "prompt_with_CoT = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"Question: If a baker bakes 20 cookies and sells 12, then bakes 10 more, how many cookies does the baker have now?\\n\\nLet's think step by step.\")\n",
    "    ])\n",
    "prompt_complex_cot = ChatPromptTemplate.from_messages([\n",
    "        (\"human\", \"Question: A is older than B. B is older than C. C is older than D. Who is the youngest?\\n\\nLet's break this down step by step to find the answer.\")\n",
    "    ])\n",
    "\n",
    "response_without_CoT = llm.invoke(prompt_without_CoT.format_messages())\n",
    "print(\"Response without CoT:\", response_without_CoT.content,\"\\n\\n\")\n",
    "\n",
    "response_with_CoT = llm.invoke(prompt_with_CoT.format_messages())\n",
    "print(\"Response with CoT:\", response_with_CoT.content,\"\\n\\n\")\n",
    "\n",
    "response_complex_cot = llm.invoke(prompt_complex_cot.format_messages())\n",
    "print(\"Response with complex CoT:\", response_complex_cot.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498210cc",
   "metadata": {},
   "source": [
    "## Tree of Thoughts ToT\n",
    "An extension of CoT, ToT allows the LLM to explore multiple reasoning paths (a \"tree\" of thoughts) and self-evaluate them, pruning less promising paths. This is even more powerful for highly complex problems but also more computationally intensive.\n",
    "\n",
    "It typically requires an agentic loop, where the LLM generates multiple \"thought candidates,\" an evaluation mechanism (which might be another LLM call or a heuristic), and a selection process to decide which path to pursue. It often involves a search algorithm (like breadth-first search or depth-first search) over the thought tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219aeaaa",
   "metadata": {},
   "source": [
    "### Explanation of the Conceptual ToT Code:\n",
    "<ol>\n",
    "<li> thought_generation_prompt: This prompt is designed to make the LLM generate multiple distinct ideas for the same problem. We run it three times in parallel to simulate different \"branches\" of thought.</li>\n",
    "\n",
    "<li>thought_branches = RunnableParallel(...): This is the core of the branching. It sends the same input (the problem statement, implicitly from the prompt) to three different chat_model_tot calls, each generating a \"thought.\" The outputs are collected into a dictionary.</li>\n",
    "\n",
    "<li>evaluate_thoughts function: This is our simplified \"evaluation\" step. In a real ToT, this might involve another LLM call to critique each thought, a complex scoring function, or even running a small simulation. Here, we simply pick the longest thought as a proxy for \"most detailed.\"</li>\n",
    "\n",
    "<li>RunnablePassthrough.assign(chosen_thought=evaluate_and_select): This crucial LCEL construct takes the output of thought_branches (the dictionary of thoughts), passes it to evaluate_and_select (which picks the best thought), and then adds this chosen_thought to the dictionary that flows to the next step.</li>\n",
    "\n",
    "<li>refinement_prompt and refine_plan: This final step takes the chosen_thought and asks the LLM to elaborate on it, simulating the \"refinement\" aspect of ToT.</li>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
