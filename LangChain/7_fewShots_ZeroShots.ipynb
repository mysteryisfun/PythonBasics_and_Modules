{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ce79189",
   "metadata": {},
   "source": [
    "### Zero-Shot Prompting:\n",
    " You provide the instruction and the input, and the LLM generates the output without any examples. This is the simplest approach.<br>\n",
    "### Few Shot Prompting:\n",
    "Few-Shot Prompting: You provide a few examples of input-output pairs along with the instruction. This helps the LLM understand the desired format, style, or task, especially for tasks that are not immediately obvious or require a specific pattern.<br>\n",
    "**FewShotPromptTemplate(), FewShotChatMessagePromptTemplate()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bdbb549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "google_apii_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "llm= ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    google_api_key=google_apii_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b867dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant use the given exmaple for reference.\n",
      "What is the capital of France?\n",
      "The capital of France is Paris.\n",
      "What is the largest planet in our solar system?\n",
      "The largest planet in our solar system is Jupiter.\n",
      "What is the tallest mountain in the world?\n",
      "LLM Response: The tallest mountain in the world, measured from base to peak, is Mount Everest.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import FewShotChatMessagePromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import HumanMessagePromptTemplate, AIMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "    AIMessagePromptTemplate.from_template(\"{output}\")\n",
    "])\n",
    "        \n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=[\n",
    "        {\n",
    "            \"input\": \"What is the capital of France?\",\n",
    "            \"output\": \"The capital of France is Paris.\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"What is the largest planet in our solar system?\",\n",
    "            \"output\": \"The largest planet in our solar system is Jupiter.\"\n",
    "        },\n",
    "    ],\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"You are a helpful assistant use the given exmaple for reference.\"),\n",
    "    few_shot_prompt,\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "])\n",
    "input_text = \"What is the tallest mountain in the world?\"\n",
    "response = final_prompt.format_messages(input=input_text)\n",
    "for message in response:\n",
    "    print(message.content)\n",
    "    \n",
    "llm_response= llm.invoke(response)\n",
    "print(\"LLM Response:\", llm_response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee1d30f",
   "metadata": {},
   "source": [
    "## Chain of Thoughts CoT <br>\n",
    "Chain of Thought (CoT) Prompting: You instruct the LLM to show its reasoning steps before providing the final answer. This helps the LLM break down complex problems, improves accuracy on reasoning tasks, and makes the LLM's thought process transparent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e57c0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response without CoT: Here's how to solve the problem:\n",
      "\n",
      "*   **Start:** The baker has 20 cookies.\n",
      "*   **Sells:** The baker sells 12 cookies, leaving 20 - 12 = 8 cookies.\n",
      "*   **Bakes more:** The baker bakes 10 more cookies, so they have 8 + 10 = 18 cookies.\n",
      "\n",
      "**Answer:** The baker has 18 cookies now. \n",
      "\n",
      "\n",
      "Response with CoT: Okay, let's break it down:\n",
      "\n",
      "1. **Starts with:** The baker begins with 20 cookies.\n",
      "2. **Sells some:** The baker sells 12 cookies, so 20 - 12 = 8 cookies are left.\n",
      "3. **Bakes more:** The baker bakes 10 more cookies, so 8 + 10 = 18 cookies.\n",
      "\n",
      "**Answer:** The baker has 18 cookies now. \n",
      "\n",
      "\n",
      "Response with complex CoT: Okay, let's break it down:\n",
      "\n",
      "*   **A is older than B:** This means B is younger than A.\n",
      "*   **B is older than C:** This means C is younger than B.\n",
      "*   **C is older than D:** This means D is younger than C.\n",
      "\n",
      "So we have A > B > C > D (where \">\" means \"is older than\").\n",
      "\n",
      "Therefore, **D** is the youngest.\n"
     ]
    }
   ],
   "source": [
    "prompt_without_CoT = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"Question: If a baker bakes 20 cookies and sells 12, then bakes 10 more, how many cookies does the baker have now?\")\n",
    "    ])\n",
    "prompt_with_CoT = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"Question: If a baker bakes 20 cookies and sells 12, then bakes 10 more, how many cookies does the baker have now?\\n\\nLet's think step by step.\")\n",
    "    ])\n",
    "prompt_complex_cot = ChatPromptTemplate.from_messages([\n",
    "        (\"human\", \"Question: A is older than B. B is older than C. C is older than D. Who is the youngest?\\n\\nLet's break this down step by step to find the answer.\")\n",
    "    ])\n",
    "\n",
    "response_without_CoT = llm.invoke(prompt_without_CoT.format_messages())\n",
    "print(\"Response without CoT:\", response_without_CoT.content,\"\\n\\n\")\n",
    "\n",
    "response_with_CoT = llm.invoke(prompt_with_CoT.format_messages())\n",
    "print(\"Response with CoT:\", response_with_CoT.content,\"\\n\\n\")\n",
    "\n",
    "response_complex_cot = llm.invoke(prompt_complex_cot.format_messages())\n",
    "print(\"Response with complex CoT:\", response_complex_cot.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498210cc",
   "metadata": {},
   "source": [
    "## Tree of Thoughts ToT\n",
    "An extension of CoT, ToT allows the LLM to explore multiple reasoning paths (a \"tree\" of thoughts) and self-evaluate them, pruning less promising paths. This is even more powerful for highly complex problems but also more computationally intensive.\n",
    "\n",
    "It typically requires an agentic loop, where the LLM generates multiple \"thought candidates,\" an evaluation mechanism (which might be another LLM call or a heuristic), and a selection process to decide which path to pursue. It often involves a search algorithm (like breadth-first search or depth-first search) over the thought tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b581390",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
