{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ce79189",
   "metadata": {},
   "source": [
    "### Zero-Shot Prompting:\n",
    " You provide the instruction and the input, and the LLM generates the output without any examples. This is the simplest approach.<br>\n",
    "### Few Shot Prompting:\n",
    "Few-Shot Prompting: You provide a few examples of input-output pairs along with the instruction. This helps the LLM understand the desired format, style, or task, especially for tasks that are not immediately obvious or require a specific pattern.<br>\n",
    "**FewShotPromptTemplate(), FewShotChatMessagePromptTemplate()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bdbb549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ujwal\\OneDrive\\Documents\\Python modules\\PythonBasics_and_Modules\\LangChain\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "google_apii_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "llm= ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    google_api_key=google_apii_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b867dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant use the given exmaple for reference.\n",
      "What is the capital of France?\n",
      "The capital of France is Paris.\n",
      "What is the largest planet in our solar system?\n",
      "The largest planet in our solar system is Jupiter.\n",
      "What is the tallest mountain in the world?\n",
      "LLM Response: The tallest mountain in the world, measured from base to peak, is Mount Everest.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import FewShotChatMessagePromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import HumanMessagePromptTemplate, AIMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "    AIMessagePromptTemplate.from_template(\"{output}\")\n",
    "])\n",
    "        \n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=[\n",
    "        {\n",
    "            \"input\": \"What is the capital of France?\",\n",
    "            \"output\": \"The capital of France is Paris.\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"What is the largest planet in our solar system?\",\n",
    "            \"output\": \"The largest planet in our solar system is Jupiter.\"\n",
    "        },\n",
    "    ],\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"You are a helpful assistant use the given exmaple for reference.\"),\n",
    "    few_shot_prompt,\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "])\n",
    "input_text = \"What is the tallest mountain in the world?\"\n",
    "response = final_prompt.format_messages(input=input_text)\n",
    "for message in response:\n",
    "    print(message.content)\n",
    "    \n",
    "llm_response= llm.invoke(response)\n",
    "print(\"LLM Response:\", llm_response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee1d30f",
   "metadata": {},
   "source": [
    "## Chain of Thoughts CoT <br>\n",
    "Chain of Thought (CoT) Prompting: You instruct the LLM to show its reasoning steps before providing the final answer. This helps the LLM break down complex problems, improves accuracy on reasoning tasks, and makes the LLM's thought process transparent.\n",
    "## Tree of Thoughts ToT\n",
    "An extension of CoT, ToT allows the LLM to explore multiple reasoning paths (a \"tree\" of thoughts) and self-evaluate them, pruning less promising paths. This is even more powerful for highly complex problems but also more computationally intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e57c0cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
