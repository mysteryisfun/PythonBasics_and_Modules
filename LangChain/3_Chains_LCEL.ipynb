{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bce4e05",
   "metadata": {},
   "source": [
    "## LangChain Expression Language (LCEL) for constituting chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae4cc8d",
   "metadata": {},
   "source": [
    "### chains are composed of **runnables** including models, prompts, parsers etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62f3441",
   "metadata": {},
   "source": [
    "### parellelism, aSync, Batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fa8faaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected argument 'gemini_api_key' provided to ChatGoogleGenerativeAI. Did you mean: 'google_api_key'?\n",
      "c:\\Users\\ujwal\\OneDrive\\Documents\\Python modules\\PythonBasics_and_Modules\\LangChain\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3699: UserWarning: WARNING! gemini_api_key is not default parameter.\n",
      "                gemini_api_key was transferred to model_kwargs.\n",
      "                Please confirm that gemini_api_key is what you intended.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "google_api_key = load_dotenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "model=\"gemini-2.0-flash\"\n",
    "parser= StrOutputParser()\n",
    "\n",
    "llm= ChatGoogleGenerativeAI(model=model, gemini_api_key=google_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fa122c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joke: Why are cats such bad dancers?  Because they have two left feet... and two more!\n",
      "Joke: Why are dogs such bad dancers?  Because they have two left feet!\n"
     ]
    }
   ],
   "source": [
    "prompt= ChatPromptTemplate.from_template(\n",
    "    \"tell me s short joke about {topic}\"\n",
    ")\n",
    "joke_chain= prompt | llm | parser\n",
    "\n",
    "response= joke_chain.invoke({\"topic\": \"cats\"})\n",
    "print(\"Joke:\", response)\n",
    "\n",
    "response2= joke_chain.invoke({\"topic\": \"dogs\"})\n",
    "print(\"Joke:\", response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47616703",
   "metadata": {},
   "source": [
    "### pydantic parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b06d7166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"name\": {\"description\": \"Name of the recipe\", \"title\": \"Name\", \"type\": \"string\"}, \"ingredients\": {\"description\": \"List of ingredients\", \"items\": {\"type\": \"string\"}, \"title\": \"Ingredients\", \"type\": \"array\"}, \"instructions\": {\"description\": \"Instructions to prepare the recipe\", \"title\": \"Instructions\", \"type\": \"string\"}, \"prep_time_minutes\": {\"description\": \"Preparation time in minutes\", \"title\": \"Prep Time Minutes\", \"type\": \"integer\"}}, \"required\": [\"name\", \"ingredients\", \"instructions\", \"prep_time_minutes\"]}\n",
      "```\n",
      "Recipe Name: Garlic Parmesan Pasta\n",
      "Ingredients: ['1 pound pasta (spaghetti, linguine, or fettuccine)', '1/4 cup butter', '4 cloves garlic, minced', '1/2 cup grated Parmesan cheese', '1/4 cup chopped fresh parsley', 'Salt and pepper to taste']\n",
      "Instructions: Cook pasta according to package directions.  While pasta cooks, melt butter in a large skillet over medium heat. Add minced garlic and cook for 1 minute, or until fragrant.  Drain pasta and add it to the skillet with the garlic butter. Stir in Parmesan cheese and parsley. Season with salt and pepper to taste. Serve immediately.\n",
      "Preparation Time (minutes): 15\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from typing import List\n",
    "\n",
    "class recipe(BaseModel):\n",
    "    name: str = Field(description=\"Name of the recipe\")\n",
    "    ingredients: List[str] = Field(description=\"List of ingredients\")\n",
    "    instructions: str = Field(description=\"Instructions to prepare the recipe\")\n",
    "    prep_time_minutes: int = Field(description=\"Preparation time in minutes\")\n",
    "\n",
    "parser= PydanticOutputParser(pydantic_object=recipe)\n",
    "format_isntruction= parser.get_format_instructions()\n",
    "print(format_isntruction)\n",
    "\n",
    "prompt= ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant that provides recipes.\"),\n",
    "        (\"human\", \"Give me a recipe for {dish} that takes less than {time} minutes to prepare.\\n {format_instructions}\"),\n",
    "    ]\n",
    "    ).partial(format_instructions=format_isntruction)\n",
    "\n",
    "recipe_chain= prompt | llm | parser\n",
    "response= recipe_chain.invoke({\"dish\": \"pasta\", \"time\": 30})\n",
    "print(\"Recipe Name:\", response.name)\n",
    "print(\"Ingredients:\", response.ingredients)\n",
    "print(\"Instructions:\", response.instructions)\n",
    "print(\"Preparation Time (minutes):\", response.prep_time_minutes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e8e4e4",
   "metadata": {},
   "source": [
    "### Streaming: parser runs only on full results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68ad158f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='```' additional_kwargs={} response_metadata={'safety_ratings': []} id='run--ef27ffde-47ef-44d0-8758-254d89607142' usage_metadata={'input_tokens': 284, 'output_tokens': 0, 'total_tokens': 284, 'input_token_details': {'cache_read': 0}}content='json\\n{\\n  \"name\": \"Quick Lemon Garlic Shrimp Pasta\",\\n' additional_kwargs={} response_metadata={'safety_ratings': []} id='run--ef27ffde-47ef-44d0-8758-254d89607142' usage_metadata={'output_tokens': 0, 'input_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}}content='  \"ingredients\": [\\n    \"1 pound linguine or spaghetti\",\\n' additional_kwargs={} response_metadata={'safety_ratings': []} id='run--ef27ffde-47ef-44d0-8758-254d89607142' usage_metadata={'output_tokens': 0, 'input_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}}content='    \"1 tablespoon olive oil\",\\n    \"4 cloves garlic, minced\",\\n    \"1 pound shrimp, peeled and deveined\",\\n    \"1' additional_kwargs={} response_metadata={'safety_ratings': []} id='run--ef27ffde-47ef-44d0-8758-254d89607142' usage_metadata={'output_tokens': 0, 'input_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}}content='/4 cup dry white wine (optional)\",\\n    \"1/4 cup lemon juice\",\\n    \"2 tablespoons butter\",\\n    \"1/4' additional_kwargs={} response_metadata={'safety_ratings': []} id='run--ef27ffde-47ef-44d0-8758-254d89607142' usage_metadata={'output_tokens': 0, 'input_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}}content=' cup chopped fresh parsley\",\\n    \"Salt and pepper to taste\"\\n  ],\\n  \"instructions\": \"Cook pasta according to package directions.  While pasta cooks, heat olive oil in a large skillet over medium heat. Add garlic' additional_kwargs={} response_metadata={'safety_ratings': []} id='run--ef27ffde-47ef-44d0-8758-254d89607142' usage_metadata={'output_tokens': 0, 'input_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}}content=' and cook for 1 minute, until fragrant. Add shrimp and cook for 2-3 minutes per side, until pink and cooked through. If using, add white wine and cook for 1 minute, until slightly reduced. Stir in' additional_kwargs={} response_metadata={'safety_ratings': []} id='run--ef27ffde-47ef-44d0-8758-254d89607142' usage_metadata={'output_tokens': 0, 'input_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}}content=' lemon juice and butter. Season with salt and pepper. Drain pasta and add it to the skillet with the shrimp sauce. Toss to combine. Stir in parsley. Serve immediately.\",\\n  \"prep_time_minutes\": 20\\n}\\n```\\n' additional_kwargs={} response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []} id='run--ef27ffde-47ef-44d0-8758-254d89607142' usage_metadata={'output_tokens': 245, 'input_tokens': -1, 'total_tokens': 244, 'input_token_details': {'cache_read': 0}}\n",
      "Full Output: ```json\n",
      "{\n",
      "  \"name\": \"Quick Lemon Garlic Shrimp Pasta\",\n",
      "  \"ingredients\": [\n",
      "    \"1 pound linguine or spaghetti\",\n",
      "    \"1 tablespoon olive oil\",\n",
      "    \"4 cloves garlic, minced\",\n",
      "    \"1 pound shrimp, peeled and deveined\",\n",
      "    \"1/4 cup dry white wine (optional)\",\n",
      "    \"1/4 cup lemon juice\",\n",
      "    \"2 tablespoons butter\",\n",
      "    \"1/4 cup chopped fresh parsley\",\n",
      "    \"Salt and pepper to taste\"\n",
      "  ],\n",
      "  \"instructions\": \"Cook pasta according to package directions.  While pasta cooks, heat olive oil in a large skillet over medium heat. Add garlic and cook for 1 minute, until fragrant. Add shrimp and cook for 2-3 minutes per side, until pink and cooked through. If using, add white wine and cook for 1 minute, until slightly reduced. Stir in lemon juice and butter. Season with salt and pepper. Drain pasta and add it to the skillet with the shrimp sauce. Toss to combine. Stir in parsley. Serve immediately.\",\n",
      "  \"prep_time_minutes\": 20\n",
      "}\n",
      "```\n",
      "\n",
      "Parsed Recipe Name: Quick Lemon Garlic Shrimp Pasta\n",
      "Parsed Ingredients: ['1 pound linguine or spaghetti', '1 tablespoon olive oil', '4 cloves garlic, minced', '1 pound shrimp, peeled and deveined', '1/4 cup dry white wine (optional)', '1/4 cup lemon juice', '2 tablespoons butter', '1/4 cup chopped fresh parsley', 'Salt and pepper to taste']\n",
      "Parsed Instructions: Cook pasta according to package directions.  While pasta cooks, heat olive oil in a large skillet over medium heat. Add garlic and cook for 1 minute, until fragrant. Add shrimp and cook for 2-3 minutes per side, until pink and cooked through. If using, add white wine and cook for 1 minute, until slightly reduced. Stir in lemon juice and butter. Season with salt and pepper. Drain pasta and add it to the skillet with the shrimp sauce. Toss to combine. Stir in parsley. Serve immediately.\n",
      "Parsed Preparation Time (minutes): 20\n",
      "Parsed Full Output: name='Quick Lemon Garlic Shrimp Pasta' ingredients=['1 pound linguine or spaghetti', '1 tablespoon olive oil', '4 cloves garlic, minced', '1 pound shrimp, peeled and deveined', '1/4 cup dry white wine (optional)', '1/4 cup lemon juice', '2 tablespoons butter', '1/4 cup chopped fresh parsley', 'Salt and pepper to taste'] instructions='Cook pasta according to package directions.  While pasta cooks, heat olive oil in a large skillet over medium heat. Add garlic and cook for 1 minute, until fragrant. Add shrimp and cook for 2-3 minutes per side, until pink and cooked through. If using, add white wine and cook for 1 minute, until slightly reduced. Stir in lemon juice and butter. Season with salt and pepper. Drain pasta and add it to the skillet with the shrimp sauce. Toss to combine. Stir in parsley. Serve immediately.' prep_time_minutes=20\n"
     ]
    }
   ],
   "source": [
    "full_output=\"\"\n",
    "recipe_chain= prompt | llm \n",
    "for chunk in recipe_chain.stream({\"dish\": \"pasta\", \"time\": 30}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "    full_output += chunk.content\n",
    "print(\"\\nFull Output:\", full_output)\n",
    "\n",
    "parsed_response= parser.parse(full_output)\n",
    "print(\"Parsed Recipe Name:\", parsed_response.name)\n",
    "print(\"Parsed Ingredients:\", parsed_response.ingredients)\n",
    "print(\"Parsed Instructions:\", parsed_response.instructions)\n",
    "print(\"Parsed Preparation Time (minutes):\", parsed_response.prep_time_minutes)\n",
    "print(\"Parsed Full Output:\", parsed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8ec1d6",
   "metadata": {},
   "source": [
    "## Input/Output Schemas and RunnablePassthrough "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114e8409",
   "metadata": {},
   "source": [
    "### Every Runnable in LCEL has defined input and output schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca885ada",
   "metadata": {},
   "source": [
    "RunnablePassThrough:<br> Without RunnablePassthrough: You lose your original data when you transform it<br>\n",
    "With RunnablePassthrough: You keep your original data AND get new computed data<br><br>Without RunnablePassthrough:\n",
    "Input: \"apple\" → [make uppercase] → Output: \"APPLE\" (lost original)<br>\n",
    "With RunnablePassthrough:\n",
    "Input: {\"fruit\": \"apple\"} → [keep original + add uppercase] → Output: {\"fruit\": \"apple\", \"uppercase\": \"APPLE\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f287720b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: An interesting related fact is that **Gustave Eiffel's company wasn't originally intended to build the Eiffel Tower;  they won a design competition against over 100 other submissions.**  This highlights the competitive nature of its creation and the unexpected path to its construction.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "summary_prompt= ChatPromptTemplate.from_template(\n",
    "    \"Summarize the following text:\\n{text}\\n\\nSummary:\"\n",
    ")\n",
    "question_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Based on the original topic '{original_topic}', what is a related interesting fact?\"\n",
    ")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "summary_chain = summary_prompt | llm | parser\n",
    "# Use RunnablePassthrough to keep the original 'topic' and add the 'summary'\n",
    "full_chain=(\n",
    "    {\"summary\":summary_chain,\"original_topic\":RunnablePassthrough()}\n",
    "    | question_prompt\n",
    "    | llm\n",
    "    | parser\n",
    ")\n",
    "input_data = {\n",
    "    \"text\": \"The Eiffel Tower is a wrought-iron lattice tower\",\n",
    "    \"original_topic\": \"Eiffel Tower\"\n",
    "}\n",
    "response = full_chain.invoke(input_data)\n",
    "print(\"Summary:\", response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2202ed",
   "metadata": {},
   "source": [
    ".assign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8142c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passthrough Result: hello world\n",
      "{'text': 'hello world', 'upper_case': 'HELLO WORLD', 'length': 11}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "passthroug= RunnablePassthrough()\n",
    "\n",
    "#result=passthroug.invoke(\"hello world\")\n",
    "result=\"hello world\"\n",
    "print(\"Passthrough Result:\", result)\n",
    "\n",
    "chain=RunnablePassthrough.assign(\n",
    "    upper_case=lambda x: x[\"text\"].upper(),\n",
    "    length=lambda x: len(x[\"text\"]),\n",
    "    )\n",
    "result=chain.invoke({\"text\": \"hello world\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3c29d8",
   "metadata": {},
   "source": [
    "## Branching <br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819a3b05",
   "metadata": {},
   "source": [
    "### Branching allows you to split execution into multiple parallel paths, each processing the same input differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d9903d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'summary': AIMessage(content='The Eiffel Tower is a wrought-iron structure located in Paris, France.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--26b5c2e5-c6e0-43ac-a1e4-b6418d77b295-0', usage_metadata={'input_tokens': 30, 'output_tokens': 16, 'total_tokens': 46, 'input_token_details': {'cache_read': 0}}), 'keywords': AIMessage(content='Keywords: Eiffel Tower, wrought-iron, lattice tower, Champ de Mars, Paris, France', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--44b4a6ed-f5a1-426c-ab38-e2999d893fb4-0', usage_metadata={'input_tokens': 31, 'output_tokens': 20, 'total_tokens': 51, 'input_token_details': {'cache_read': 0}}), 'sentiment': AIMessage(content='Sentiment: Neutral.\\n\\nThe text provides factual information about the Eiffel Tower, without expressing any positive or negative opinions or emotions.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--79775cec-6a62-4687-bdbe-5ebd10db3993-0', usage_metadata={'input_tokens': 32, 'output_tokens': 26, 'total_tokens': 58, 'input_token_details': {'cache_read': 0}}), 'original_text': {'text': 'The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.', 'original_text': 'Eiffel Tower'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "summary_prompt= PromptTemplate.from_template(\n",
    "    \"Summarize the following text:\\n{text}\\n\\nSummary:\"\n",
    ")\n",
    "keywords_prompt= PromptTemplate.from_template(\n",
    "    \"Extract keywords from the following text:\\n{text}\\n\\nKeywords:\"    \n",
    ")\n",
    "sentiment_prompt= PromptTemplate.from_template(\n",
    "    \"Analyze the sentiment of the following text:\\n{text}\\n\\nSentiment:\"\n",
    ")\n",
    "\n",
    "branch_chain= RunnableParallel({\n",
    "    \"summary\": summary_prompt | llm,\n",
    "    \"keywords\": keywords_prompt | llm,\n",
    "    \"sentiment\": sentiment_prompt | llm,\n",
    "    \"original_text\": RunnablePassthrough()\n",
    "})\n",
    "\n",
    "result=branch_chain.invoke({\n",
    "    \"text\": \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.\",\n",
    "    \"original_text\": \"Eiffel Tower\"\n",
    "})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d70663d",
   "metadata": {},
   "source": [
    "output is stored in dict with the name of runnable path as key and output as a value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60eeb337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: Joke: Why are cats such bad dancers?  Because they have two left feet!\n",
      "Fact: A cat's purr is not just a cute sound;  the frequency of a cat's purr (25-150 Hz) has been shown to promote bone and tissue healing in humans and cats themselves.  This is why you might find a purring cat recovering from injury more quickly.\n"
     ]
    }
   ],
   "source": [
    "joke_prompt= ChatPromptTemplate.from_template(\n",
    "    \"tell me a short joke about {topic}\"\n",
    ")\n",
    "\n",
    "fact_prompt= ChatPromptTemplate.from_template(\n",
    "    \"tell me an interesting fact about {topic}\"\n",
    ")\n",
    "\n",
    "parellel_chain=RunnableParallel(\n",
    "    joke=joke_prompt | llm | parser,\n",
    "    fact=fact_prompt | llm | parser,\n",
    ")\n",
    "\n",
    "final_chain=(\n",
    "    {\"topic\":RunnablePassthrough()}\n",
    "    | parellel_chain\n",
    "    | (lambda x: f\"Joke: {x['joke']}\\nFact: {x['fact']}\")\n",
    ")\n",
    "\n",
    "topic = \"cats\"\n",
    "response = final_chain.invoke({\"topic\": topic})\n",
    "print(\"Final Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbc5d109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch Response (Cat): content=\"Why don't cats play poker in the jungle?\\n\\nToo many cheetahs!\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--7c5b3381-c674-4fda-a227-4601c3a7c6af-0' usage_metadata={'input_tokens': 7, 'output_tokens': 19, 'total_tokens': 26, 'input_token_details': {'cache_read': 0}}\n",
      "Branch Response (Dog): content='Why don\\'t scientists trust atoms?\\n\\nBecause they make up everything!\\n\\n...Including my dog\\'s excuses for eating the sofa. He says it was an \"atomic reaction.\"' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--5a9be59c-60ba-4d63-a8c9-c009bb33979f-0' usage_metadata={'input_tokens': 7, 'output_tokens': 38, 'total_tokens': 45, 'input_token_details': {'cache_read': 0}}\n",
      "Branch Response (Default): content=\"That's a classic! It's so simple, it's almost profound. Thanks for sharing!\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--6f3e3ce7-15a8-4179-8bae-ae226892da68-0' usage_metadata={'input_tokens': 34, 'output_tokens': 23, 'total_tokens': 57, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "# Define prompts\n",
    "cat_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a joke about {animal}.\"\n",
    ")\n",
    "dog_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a joke about {animal}.\"\n",
    ")\n",
    "\n",
    "# Create a default prompt for other animals\n",
    "default_prompt = ChatPromptTemplate.from_template(\n",
    "    \"I don't have a specific joke for {animal}, but here's a general one: Why did the chicken cross the road? To get to the other side!\"\n",
    ")\n",
    "\n",
    "# Create chains\n",
    "cat_chain = cat_prompt | llm\n",
    "dog_chain = dog_prompt | llm\n",
    "default_chain = default_prompt | llm\n",
    "\n",
    "# Create RunnableBranch with default\n",
    "branch_chain = RunnableBranch(\n",
    "    (lambda x: \"cat\" in x[\"animal\"].lower(), cat_chain),\n",
    "    (lambda x: \"dog\" in x[\"animal\"].lower(), dog_chain),\n",
    "    default_chain  # This is the required default branch\n",
    ")\n",
    "\n",
    "# Test the branch\n",
    "response = branch_chain.invoke({\"animal\": \"cat\"})\n",
    "print(\"Branch Response (Cat):\", response)\n",
    "\n",
    "response = branch_chain.invoke({\"animal\": \"dog\"})\n",
    "print(\"Branch Response (Dog):\", response)\n",
    "\n",
    "response = branch_chain.invoke({\"animal\": \"elephant\"})\n",
    "print(\"Branch Response (Default):\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37081a22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
