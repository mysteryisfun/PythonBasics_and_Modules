{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1db5d829",
   "metadata": {},
   "source": [
    "For llm bulk output invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2c813c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google API Key loaded successfully.\n",
      "Prompt: what is the importance of python in llms architecture\n",
      "Response: Python plays a crucial role in the architecture and development of Large Language Models (LLMs) due to several key reasons:\n",
      "\n",
      "**1. Ecosystem of Powerful Libraries:**\n",
      "\n",
      "*   **TensorFlow and PyTorch:** These are the dominant deep learning frameworks used for building and training LLMs. Both have excellent Python APIs, making it easy to define neural network architectures, implement training loops, and perform inference. Python's ease of use allows researchers and engineers to focus on the model architecture and training process rather than struggling with low-level implementation details.\n",
      "*   **NumPy:**  Fundamental for numerical computation, NumPy provides efficient array operations and mathematical functions essential for handling tensors (multi-dimensional arrays) used in deep learning.\n",
      "*   **Pandas:**  Used for data manipulation and analysis, Pandas is valuable for preparing and processing the massive datasets required for training LLMs. It simplifies tasks like cleaning data, handling missing values, and creating data pipelines.\n",
      "*   **Scikit-learn:** Although not directly used for building LLM architectures, scikit-learn is valuable for various pre- and post-processing tasks, such as feature extraction, model evaluation, and dimensionality reduction.\n",
      "*   **Hugging Face Transformers:** This library provides pre-trained models, tokenizers, and utilities that significantly simplify the development and deployment of LLMs. It's a central hub for accessing and using pre-trained models like BERT, GPT, and T5, and it provides a Python interface for fine-tuning these models on specific tasks.\n",
      "*   **Other NLP Libraries (NLTK, SpaCy):** While Transformers is often preferred for modern LLMs, these libraries offer useful tools for text processing, such as tokenization, stemming, and part-of-speech tagging, which can be used in conjunction with LLMs.\n",
      "\n",
      "**2. Ease of Use and Readability:**\n",
      "\n",
      "*   Python's clean syntax and readability make it easier to write, understand, and maintain code, which is crucial for complex projects like LLMs. This is especially important in research environments where code is frequently shared and iterated upon.\n",
      "*   Python's dynamic typing allows for rapid prototyping and experimentation, which is valuable during the development and refinement of LLM architectures.\n",
      "\n",
      "**3. Large and Active Community:**\n",
      "\n",
      "*   Python has a massive and active community of developers and researchers. This means there are plenty of resources available online, including tutorials, documentation, and forums, making it easier to learn and troubleshoot problems.\n",
      "*   The community contributes to the development and maintenance of the numerous libraries mentioned above, ensuring that they remain up-to-date and well-supported.\n",
      "\n",
      "**4. Versatility and Integration:**\n",
      "\n",
      "*   Python can be easily integrated with other technologies and systems. This is important for deploying LLMs in real-world applications, where they may need to interact with databases, web servers, and other software components.\n",
      "*   Python can be used for various tasks related to LLMs, including data preprocessing, model training, evaluation, deployment, and API development.\n",
      "\n",
      "**5. Rapid Prototyping and Experimentation:**\n",
      "\n",
      "*   Python allows for quick experimentation with different LLM architectures, training techniques, and hyperparameters.  This is essential in the rapidly evolving field of LLMs, where new ideas and approaches are constantly being explored.\n",
      "*   The availability of pre-trained models and libraries makes it easier to build and test new LLM-based applications without having to start from scratch.\n",
      "\n",
      "**In Summary:**\n",
      "\n",
      "Python's extensive ecosystem of libraries, ease of use, large community, versatility, and support for rapid prototyping make it the dominant language in the development, training, and deployment of Large Language Models. Without Python, the progress and accessibility of LLMs would be significantly hindered. It allows researchers and engineers to focus on the core challenges of building intelligent systems rather than being bogged down by low-level implementation details.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "\n",
    "load_dotenv()\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(\"Google API Key loaded successfully.\")\n",
    "else:\n",
    "    print(\"Failed to load Google API Key. Please check your .env file.\")\n",
    "    exit()\n",
    "llm= GoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    google_api_key=google_api_key,\n",
    ")\n",
    "\n",
    "prompt=\"what is the importance of python in llms architecture\"\n",
    "\n",
    "print(\"Prompt:\", prompt)\n",
    "response=llm.invoke(prompt)\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fddf7d5",
   "metadata": {},
   "source": [
    "For LLm stream output chunks in llm.stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401be361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is absolutely crucial in the architecture and development of Large Language Models (LLMs) for several key reasons:\n",
      "\n",
      "**1. Ecosystem of Libraries and Frameworks:**\n",
      "\n",
      "*   **Deep Learning Frameworks:** Python is the primary language for popular deep learning frameworks like:\n",
      "    *   **TensorFlow:**  Developed by Google, TensorFlow offers a comprehensive ecosystem for building and deploying LLMs.\n",
      "    *   **PyTorch:** Developed by Meta (Facebook), PyTorch is known for its flexibility, dynamic computation graph, and strong community support, making it a favorite for research and development.\n",
      "    *   **Keras:** A high-level API that can run on top of TensorFlow, PyTorch, or other backends, simplifying model building and experimentation.\n",
      "    *   These frameworks provide the building blocks for defining neural network architectures, implementing training algorithms (like backpropagation), and managing computational resources.\n",
      "\n",
      "*   **Data Science Libraries:**  LLMs are trained on massive datasets. Python's data science libraries are essential for data preparation, cleaning, and manipulation:\n",
      "    *   **NumPy:** Provides efficient numerical computation capabilities for handling large arrays and matrices, which are fundamental to deep learning.\n",
      "    *   **Pandas:**  Offers powerful data structures (DataFrames) and tools for data analysis, cleaning, and transformation.  Crucial for pre-processing text data.\n",
      "\n",
      "*   **Natural Language Processing (NLP) Libraries:** Python has excellent NLP libraries that streamline text processing:\n",
      "    *   **NLTK (Natural Language Toolkit):** A comprehensive library for tasks like tokenization, stemming, lemmatization, part-of-speech tagging, and parsing.\n",
      "    *   **spaCy:** A fast and efficient library for advanced NLP tasks, including named entity recognition, dependency parsing, and text classification.\n",
      "    *   **Transformers:**  A library from Hugging Face that provides pre-trained LLMs (like BERT, GPT, T5) and tools for fine-tuning them on specific tasks. This library has revolutionized the accessibility of LLMs.\n",
      "    *   **Gensim:** Focuses on topic modeling, document similarity, and other text analysis tasks.\n",
      "\n",
      "*   **Machine Learning Libraries:**\n",
      "    *   **Scikit-learn:**  While not directly used for building the core LLM itself, scikit-learn is valuable for auxiliary tasks like evaluating model performance, hyperparameter tuning, and building simpler models for comparison or pre-processing.\n",
      "\n",
      "**2. Ease of Use and Readability:**\n",
      "\n",
      "*   Python's syntax is relatively simple and easy to learn, making it accessible to a wide range of researchers and developers.  This reduces the barrier to entry for working with complex LLM architectures.\n",
      "*   Its readability promotes collaboration and code maintainability, essential for large-scale projects like LLM development.\n",
      "\n",
      "**3. Rapid Prototyping and Experimentation:**\n",
      "\n",
      "*   The combination of Python's ease of use and the powerful libraries mentioned above allows for rapid prototyping and experimentation. Researchers can quickly iterate on different model architectures, training strategies, and data pre-processing techniques.\n",
      "*   The REPL (Read-Eval-Print Loop) environment in Python enables interactive development and debugging.\n",
      "\n",
      "**4. Community and Resources:**\n",
      "\n",
      "*   Python has a massive and active community of developers and researchers.  This means there's ample support available, including documentation, tutorials, online forums, and open-source projects.\n",
      "*   The extensive documentation and community support make it easier to troubleshoot problems, learn new techniques, and contribute to the advancement of LLM technology.\n",
      "\n",
      "**5. Scalability and Deployment:**\n",
      "\n",
      "*   Python can be used in conjunction with tools like Dask and Spark to handle large datasets and scale training across multiple machines.\n",
      "*   Frameworks like TensorFlow Serving, TorchServe, and FastAPI (with uvicorn/gunicorn) allow for efficient deployment of LLMs as APIs.\n",
      "\n",
      "**6. Pre-trained Models and Transfer Learning:**\n",
      "\n",
      "*   Many pre-trained LLMs are available in Python-friendly formats (e.g., models from Hugging Face).  This enables transfer learning, where you can fine-tune a pre-trained model on your specific task, saving significant time and resources compared to training from scratch.\n",
      "\n",
      "**In Summary:**\n",
      "\n",
      "Python provides a powerful and versatile ecosystem for all stages of LLM development, from data preparation and model building to training, evaluation, and deployment. Its ease of use, extensive libraries, and strong community have made it the dominant language in the field.  Without Python, the rapid progress in LLMs we've seen in recent years would not have been possible.\n",
      "\n",
      "Full Response: Python is absolutely crucial in the architecture and development of Large Language Models (LLMs) for several key reasons:\n",
      "\n",
      "**1. Ecosystem of Libraries and Frameworks:**\n",
      "\n",
      "*   **Deep Learning Frameworks:** Python is the primary language for popular deep learning frameworks like:\n",
      "    *   **TensorFlow:**  Developed by Google, TensorFlow offers a comprehensive ecosystem for building and deploying LLMs.\n",
      "    *   **PyTorch:** Developed by Meta (Facebook), PyTorch is known for its flexibility, dynamic computation graph, and strong community support, making it a favorite for research and development.\n",
      "    *   **Keras:** A high-level API that can run on top of TensorFlow, PyTorch, or other backends, simplifying model building and experimentation.\n",
      "    *   These frameworks provide the building blocks for defining neural network architectures, implementing training algorithms (like backpropagation), and managing computational resources.\n",
      "\n",
      "*   **Data Science Libraries:**  LLMs are trained on massive datasets. Python's data science libraries are essential for data preparation, cleaning, and manipulation:\n",
      "    *   **NumPy:** Provides efficient numerical computation capabilities for handling large arrays and matrices, which are fundamental to deep learning.\n",
      "    *   **Pandas:**  Offers powerful data structures (DataFrames) and tools for data analysis, cleaning, and transformation.  Crucial for pre-processing text data.\n",
      "\n",
      "*   **Natural Language Processing (NLP) Libraries:** Python has excellent NLP libraries that streamline text processing:\n",
      "    *   **NLTK (Natural Language Toolkit):** A comprehensive library for tasks like tokenization, stemming, lemmatization, part-of-speech tagging, and parsing.\n",
      "    *   **spaCy:** A fast and efficient library for advanced NLP tasks, including named entity recognition, dependency parsing, and text classification.\n",
      "    *   **Transformers:**  A library from Hugging Face that provides pre-trained LLMs (like BERT, GPT, T5) and tools for fine-tuning them on specific tasks. This library has revolutionized the accessibility of LLMs.\n",
      "    *   **Gensim:** Focuses on topic modeling, document similarity, and other text analysis tasks.\n",
      "\n",
      "*   **Machine Learning Libraries:**\n",
      "    *   **Scikit-learn:**  While not directly used for building the core LLM itself, scikit-learn is valuable for auxiliary tasks like evaluating model performance, hyperparameter tuning, and building simpler models for comparison or pre-processing.\n",
      "\n",
      "**2. Ease of Use and Readability:**\n",
      "\n",
      "*   Python's syntax is relatively simple and easy to learn, making it accessible to a wide range of researchers and developers.  This reduces the barrier to entry for working with complex LLM architectures.\n",
      "*   Its readability promotes collaboration and code maintainability, essential for large-scale projects like LLM development.\n",
      "\n",
      "**3. Rapid Prototyping and Experimentation:**\n",
      "\n",
      "*   The combination of Python's ease of use and the powerful libraries mentioned above allows for rapid prototyping and experimentation. Researchers can quickly iterate on different model architectures, training strategies, and data pre-processing techniques.\n",
      "*   The REPL (Read-Eval-Print Loop) environment in Python enables interactive development and debugging.\n",
      "\n",
      "**4. Community and Resources:**\n",
      "\n",
      "*   Python has a massive and active community of developers and researchers.  This means there's ample support available, including documentation, tutorials, online forums, and open-source projects.\n",
      "*   The extensive documentation and community support make it easier to troubleshoot problems, learn new techniques, and contribute to the advancement of LLM technology.\n",
      "\n",
      "**5. Scalability and Deployment:**\n",
      "\n",
      "*   Python can be used in conjunction with tools like Dask and Spark to handle large datasets and scale training across multiple machines.\n",
      "*   Frameworks like TensorFlow Serving, TorchServe, and FastAPI (with uvicorn/gunicorn) allow for efficient deployment of LLMs as APIs.\n",
      "\n",
      "**6. Pre-trained Models and Transfer Learning:**\n",
      "\n",
      "*   Many pre-trained LLMs are available in Python-friendly formats (e.g., models from Hugging Face).  This enables transfer learning, where you can fine-tune a pre-trained model on your specific task, saving significant time and resources compared to training from scratch.\n",
      "\n",
      "**In Summary:**\n",
      "\n",
      "Python provides a powerful and versatile ecosystem for all stages of LLM development, from data preparation and model building to training, evaluation, and deployment. Its ease of use, extensive libraries, and strong community have made it the dominant language in the field.  Without Python, the rapid progress in LLMs we've seen in recent years would not have been possible.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_response=\"\"\n",
    "for chunk in llm.stream(prompt):\n",
    "    print(chunk, end=\"\",flush=True) #print imediately\n",
    "    full_response += chunk\n",
    "print(\"\\nFull Response:\", full_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaabaea",
   "metadata": {},
   "source": [
    "Temperature and Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b39ec9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 0.1 Okay, here are some direct-to-the-point feature ideas for a social media app:\n",
      "\n",
      "*   **Ephemeral Groups:** Groups that automatically dissolve after a set time.\n",
      "*   **Mood-Based Filtering:** Filter content based on the\n",
      "Response 0.9 Okay, here are some feature ideas for a social media app, aiming for directness and novelty:\n",
      "\n",
      "**Focusing on Connection & Community:**\n",
      "\n",
      "*   **\"Micro-Groups\":**  Users can quickly create temporary, hyper-local groups\n"
     ]
    }
   ],
   "source": [
    "prompt=\"brainstorm about a new feature for a social media app direct to the point\"\n",
    "llm= GoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.1,\n",
    "    max_output_tokens=50\n",
    ")\n",
    "response=llm.invoke(prompt)\n",
    "print(\"Response 0.1\", response)\n",
    "\n",
    "llm= GoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.9,\n",
    "    max_output_tokens=50\n",
    ")\n",
    "response=llm.invoke(prompt)\n",
    "print(\"Response 0.9\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019a527f",
   "metadata": {},
   "source": [
    "## Converstational with mutli Type messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b972ec83",
   "metadata": {},
   "source": [
    "HumanMessage: Represents input from the user.\n",
    "\n",
    "AIMessage: Represents output from the AI model (assistant).\n",
    "\n",
    "SystemMessage: Provides high-level instructions or context to the AI model, influencing its overall behavior. This message typically comes first and sets the tone or persona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aecf29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"I'm doing well, thanks for asking! How can I help you today?\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--5d53db63-789c-49e1-a6d6-105e685bceb0-0' usage_metadata={'input_tokens': 17, 'output_tokens': 18, 'total_tokens': 35, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    google_api_key=google_api_key,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "        SystemMessage(content=\"You are a helpful assistant that provides concise answers.\"),\n",
    "        HumanMessage(content=\"hey man how are you holding up\"),\n",
    "    ]\n",
    "response = llm.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedf30c4",
   "metadata": {},
   "source": [
    "### Continous convo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4423129e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new messages: [SystemMessage(content='You are a helpful assistant that provides concise answers.', additional_kwargs={}, response_metadata={}), HumanMessage(content='hey man how are you holding up', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm doing well, thanks for asking! How can I help you today?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='what was the prev question', additional_kwargs={}, response_metadata={})]\n",
      "content='The previous question was \"hey man how are you holding up\".' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--04edb2ab-a7b6-49b7-9381-f102164b2871-0' usage_metadata={'input_tokens': 39, 'output_tokens': 14, 'total_tokens': 53, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "messages.append(AIMessage(content=response.content))\n",
    "messages.append(HumanMessage(content=input(\"new question to continue:\")))\n",
    "print(\"new messages:\", messages)\n",
    "response = llm.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b42194d",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "*embed_query()* <br>\n",
    "*embed_documents()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ac1594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: LangChain is a framework for developing applications powered by language models.\n",
      "Embedding first 5 dimensions out of 768: [0.03113183006644249, -0.0007059850031509995, -0.04631577804684639, 0.012090352363884449, 0.07094656676054001]\n",
      "Embedding dimension: 768\n",
      "Embedding for text 1 first 5 dimensions out of 768: [0.03113183006644249, -0.0007059850031509995, -0.04631577804684639, 0.012090352363884449, 0.07094656676054001]\n",
      "Embedding for text 1 dimension: 768\n",
      "Embedding for text 2 first 5 dimensions out of 768: [0.036799103021621704, -0.021485621109604836, -0.08486296236515045, -0.00466722808778286, 0.08083818107843399]\n",
      "Embedding for text 2 dimension: 768\n",
      "Embedding for text 3 first 5 dimensions out of 768: [0.033488865941762924, -0.012272308580577374, -0.054907411336898804, -0.012687521986663342, 0.05795364826917648]\n",
      "Embedding for text 3 dimension: 768\n",
      "Embedding for text 4 first 5 dimensions out of 768: [0.05609360337257385, -0.03234552964568138, -0.05605832859873772, -0.03096853382885456, 0.07759455591440201]\n",
      "Embedding for text 4 dimension: 768\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embedding_model= GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=google_api_key\n",
    "    )\n",
    "text=\"LangChain is a framework for developing applications powered by language models.\"\n",
    "print(\"Text:\", text)\n",
    "\n",
    "embedding=embedding_model.embed_query(text)\n",
    "print(\"Embedding first 5 dimensions out of 768:\", embedding[:5])\n",
    "print(\"Embedding dimension:\", len(embedding))\n",
    "\n",
    "multiple_texts=[\n",
    "    \"LangChain is a framework for developing applications powered by language models.\",\n",
    "    \"It provides a standard interface for working with different language models.\",\n",
    "    \"JavaScript is a versatile programming language.\",\n",
    "    \"It is widely used for web development.\"]\n",
    "\n",
    "multi_embedding=embedding_model.embed_documents(multiple_texts)\n",
    "\n",
    "for i, emb in enumerate(multi_embedding):\n",
    "    print(f\"Embedding for text {i+1} first 5 dimensions out of 768:\", emb[:5])\n",
    "    print(f\"Embedding for text {i+1} dimension:\", len(emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098ef2c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
